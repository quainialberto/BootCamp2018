{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alberto Quaini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_steepestDescent(Q, b, c, x_0, eps=1e-9, max_iter=1e5):\n",
    "    \"\"\" Solve quadratic optimization problem of the form\n",
    "    x^TQx -b^Tx+c, where Q is an nxn positive definite matrix,\n",
    "    b, x are vectors in R^n and c is a real number.    \n",
    "    Method: Steepest descent.\n",
    "    \"\"\"\n",
    "    n = np.size(x_0)\n",
    "    if not np.size(b) == n:\n",
    "        raise ValueError('b is not of length n (n being the size of x_0)')\n",
    "    if not Q.shape == (n,n):\n",
    "        raise ValueError('Q is not a nxn matrix, (n being the size of x_0)')\n",
    "    \n",
    "    x1 = x_0\n",
    "    criterion = eps+1\n",
    "    for i in range(int(max_iter)):\n",
    "        \n",
    "        M = Q @ x_0 - b\n",
    "        MT = M.T\n",
    "        a_num = MT @ M\n",
    "        a_den = MT @ Q @ M\n",
    "        a = a_num / a_den\n",
    "        x_1 = x_0 - a * M\n",
    "        criterion = norm(MT)\n",
    "        if (criterion < eps):\n",
    "            break\n",
    "        x_0 = x_1\n",
    "    \n",
    "    f = 0.5 * x_1.T @ Q @ x_1 - b.T @ x_1 + c\n",
    "    if (i < max_iter):\n",
    "        print('Converged')\n",
    "    else:\n",
    "        print('Did not converge!')\n",
    "    return x_1, f\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.14285714,  1.42857143]), -1.8571428571428568)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[3, 1],[1, 1.5]])\n",
    "b = np.array([-2, 1])\n",
    "c = 0\n",
    "x_0 = np.array([1.1, 1.5])\n",
    "quad_steepestDescent(Q, b, c, x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad(x, args):\n",
    "    Q, b, c = args\n",
    "    return 0.5 * x.T @ Q @ x - b.T @ x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -1.8571428571428568\n",
       " hess_inv: array([[ 0.42886341, -0.28561563],\n",
       "       [-0.28561563,  0.85717585]])\n",
       "      jac: array([5.96046448e-08, 2.98023224e-08])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 20\n",
       "      nit: 4\n",
       "     njev: 5\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-1.14285713,  1.42857143])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.minimize(quad, x_0, args=[Q, b, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_gradient(f, x, rerr=1e-8):\n",
    "    \"\"\" Compute the numerical gradient of function\n",
    "    f:R^n->R, at a given point x in R^n.\n",
    "    rerr is an estimate for the maximum relative error\n",
    "    of f near x.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = np.size(x)\n",
    "    h = np.sqrt(rerr)\n",
    "    grad = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        ei = np.eye(n)[i]\n",
    "        grad[i] = (f(x + h*ei) - f(x - h*ei)) / (2*h)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  4., -1.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: np.array([x[0]**2 + 4*x[1] - x[2]])\n",
    "x = np.array([1, 2, 3])\n",
    "num_gradient(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant_minimizer(x0, x1, f_prime, eps=1e-9):\n",
    "    \"\"\" Find the minimizer of a function \n",
    "    via the secant method.\n",
    "    Initial values: x0 and x1,\n",
    "    Accuracy: eps,\n",
    "    First derivative of f: f_prime.\n",
    "    \"\"\"\n",
    "    max_iter = int(1e4)\n",
    "    dist = eps + 1\n",
    "    xvec = [x0, x1]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        x = x1 - f_prime(x1) * (x1-x0) / ( f_prime(x1)-f_prime(x0) )\n",
    "        x0 = x1\n",
    "        x1 = x\n",
    "        xvec.append(x1)\n",
    "        dist = np.abs(x1 - x0) #/ np.abs(x0)\n",
    "        try:\n",
    "            if (dist < eps):\n",
    "                break\n",
    "        except:\n",
    "            print(x)\n",
    "            break\n",
    "    \n",
    "    if i == max_iter:\n",
    "        print('Secant method did not converge')\n",
    "        \n",
    "    return xvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepestDescent(f, x_0, eps=1e-9, max_iter=1e5):\n",
    "    \"\"\" Solve unconstrained nonlinear optimization problems\n",
    "    via the steepest descent method.\n",
    "    \"\"\"\n",
    "    n = np.size(x_0)\n",
    "    grad_f = lambda x: num_gradient(f, x)\n",
    "    f_secant = lambda a, x: grad_f( x - a*grad_f(x) ).T @ grad_f(x)\n",
    "    \n",
    "    for i in range(int(max_iter)):\n",
    "      \n",
    "        grad = grad_f(x_0)\n",
    "        f_sec = lambda a: f_secant(a, x_0)\n",
    "        a = secant_minimizer(1, 1.5, f_sec, eps)[-1]\n",
    "        x_1 = x_0 - a * grad\n",
    "        criterion = norm(x_0 - x_1)\n",
    "        if (criterion < eps):\n",
    "            break\n",
    "        x_0 = x_1\n",
    "    \n",
    "    if (i < max_iter):\n",
    "        print('Steepest descent routine converged')\n",
    "    else:\n",
    "        print('Steepest descent routine did not converge')\n",
    "        \n",
    "    return x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steepest descent routine converged\n",
      "minimizer is:  [0.99999758 0.99999516]\n",
      "minimum is:  5.855454170821065e-12\n"
     ]
    }
   ],
   "source": [
    "xvec_guess = np.array([-2,2])\n",
    "xmin = steepestDescent(rosenbrock, xvec_guess)\n",
    "print('minimizer is: ', xmin)\n",
    "print('minimum is: ', rosenbrock(xmin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YESSS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
