\documentclass[11.5pt, letterpaper, bibtotoc,
    tablecaptionabove, figurecaptionabove]{article}


\setlength{\headheight}{10pt}
\setlength{\headsep}{15pt}
\setlength{\topmargin}{-25pt}
\setlength{\topskip}{0in}
\setlength{\textheight}{8.7in}
\setlength{\footskip}{0.3in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\textwidth}{6.5in}

\usepackage{setspace}
\setstretch{1.2}
\setlength{\parskip}{5pt}%{6pt}
\setlength{\parindent}{0pt}

\usepackage{subfigure}
\usepackage{subfiles}
\usepackage{graphicx}
\usepackage{epsfig}
\graphicspath{{images/}{../images/}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{notation}{Notation}
\newtheorem{corollary}{Corollary}
\newtheorem{remarks}{Remarks}
\newtheorem{examples}{Examples}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\ri}{ri}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\essential}{ess}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\rank}{rank}

\makeatletter
\newcommand{\leqnomode}{\tagsleft@true\let\veqno\@@leqno}
\newcommand{\reqnomode}{\tagsleft@false\let\veqno\@@eqno}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert#1
\right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
\makeatother

\usepackage{bm}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=red,
}
\usepackage[margin=1in]{geometry}

\begin{document}

\textbf{Alberto Quaini}

\section*{Inner Product Spaces Exercises}

\subsection*{Exercise 1}
(i)
\begin{align*}
    &\left( ||x+y||^2 - ||x-y||^2 \right)/4=\\
    &\left( <x,x> + <y,y> + 2<x,y> - <x,x> - <y,y> + 2<x,y>\right)/4=\\
    &<x,y>.
\end{align*}
(ii)
\begin{align*}
    &\left( ||x+y||^2 + ||x-y||^2 \right) / 4 =\\
    &\left( <x,x> + <y,y> + 2<x,y> + <x,x> + <y,y> - 2<x,y>\right) / 2 =\\
    &<x,x> + <y,y>.
\end{align*}

\subsection*{Exercise 2}
\begin{align*}
    &(||x+y||^2 - ||x-y||^2 + i||x-iy||^2 - i||x+iy||^2) / 4 =\\
    &(<x+y,x+y> - <x-y,x-y> + i<x-iy,y-iy> - i<x+iy,x+iy>) / 4 =\\ 
    &(2<x,y> + 2<y,x> -2<x,y> +2<y,x>) / 4=\\
    &<x,y>.
\end{align*}

\subsection*{Exercise 3}
(i)
$<x,x^5> = \int_0^1x^6dx=x^7/7|_0^1=1/7$,
$||x|| = \int_0^1x^2dx=x^3/3|_0^1=1/3$ and
$||x^5|| = \int_0^1x^10dx=x^11/11|_0^1=1/11$.
Therefore $\cos\theta=\sqrt{33}/7$ implies $\theta=34.5$.

(ii)
\begin{align*}
    \frac{<f,g>}{||f||||g||}=\frac{<x,x^5>}{||x||||x^5||}=
    \frac{1/7}{\sqrt{1/(3\cdot11)}}=\frac{\sqrt{33}}{7}.
\end{align*}
Therefore $\theta=0.608$.

\subsection*{Exercise 8}
(i)
\begin{align*}
    ||\cos(t)||=\frac{1}{\pi}\int_{-\pi}^\pi\cos^2(t)dt=
    \frac{1}{\pi}\left.\frac{\cos(x)\sin(x)-x}{2}\right\lvert_{-\pi}^pi=\frac{\pi}{\pi}=1,
\end{align*}
and similarly $||\sin(t)||=1$.
Also
\begin{align*}
    ||\cos(2t)||=\frac{1}{\pi}\int_{-\pi}^\pi\cos^2(2t)dt=
    \frac{1}{\pi}\left.\frac{\sin(4t)+4t}{8}\right\lvert_{-\pi}^pi=\frac{\pi}{\pi}=1,
\end{align*}
and similarly $||\sin(2t)||=1$.
Therefore the basis is normalized.

The following integrals:
\begin{align*}
    <\cos(t),\sin(t)> = \frac{1}{\pi}\int_{-\pi}^\pi\cos(t)\sin(t)dt=
    \frac{1}{\pi}\left.\frac{\sin^2(x)}{x}\right\lvert_{-\pi}^pi=0,
\end{align*}

\begin{align*}
    <\cos(t),\cos(2t)> = \frac{1}{\pi}\int_{-\pi}^\pi\cos(t)\cos(2t)dt=
    \frac{1}{\pi}\left.frac{3\sin(t)-2\sin^3(t)}{3}\right\lvert_{-\pi}^pi=0,
\end{align*}

\begin{align*}
    <\cos(t),\sin(2t)> = \frac{1}{\pi}\int_{-\pi}^\pi\cos(t)\sin(2t)dt=
    \frac{1}{\pi}\left.frac{-2\cos^3(t)}{3}\right\lvert_{-\pi}^pi=0,
\end{align*}

\begin{align*}
    <\cos(2t),\sin(2t)> = \frac{1}{\pi}\int_{-\pi}^\pi\cos(2t)\sin(2t)dt=
    \frac{1}{\pi}\left.frac{-\cos^2(2t)}{4}\right\lvert_{-\pi}^pi=0,
\end{align*}

and so on, shows that $S$ is an orthonormal basis.

(ii)
\begin{align*}
    ||t|| = \sqrt{\frac{1}{\pi}\int_{-\pi}^\pi t^2dt}=
    \frac{1}{\sqrt{\pi}}\sqrt{\pi^3/3+\pi^3/3}=
    \pi\sqrt{2/3}.
\end{align*}

(iii)
Since $<x,\cos(3x)>=0$ for any $x\in S$, $\text{proj}_X(\cos(3x))=0$.

(iv)
\begin{align*}
    &<\sin(t),t>=\sin(t)-t\cos(t)\lvert_{-\pi}^\pi=2\pi,\\
    &<\cos(t),t>=t\sin(t)-\cos(t)\lvert_{-\pi}^\pi=0,\\
    &<\cos(2t),t>=(2t\sin(2t)+\cos(2t))/4\lvert_{-\pi}^\pi=0,\ \text{and finally}\\
    &<\sin(2t),t>\sin\left(2x\right)-2x\cos\left(2x\right)/4\lvert_{-\pi}^\pi=-\pi.
\end{align*}
Therefore, $\text{proj}_X(t)=2\pi\sin(t)-\pi\sin(2t)$

\subsection*{Exercise 9}
A rotation of angle $\theta$ in $\mathbb R^2$ represented as a matrix $R$ 
in the standard basis is an orthonormal transformation since
\[
    \begin{bmatrix}
        \cos(\theta) & -\sin(\theta)\\
        \sin(\theta) & \cos(\theta)
    \end{bmatrix}^T
    \begin{bmatrix}
       \cos(\theta) & -\sin(\theta)\\
       \sin(\theta) & \cos(\theta)
    \end{bmatrix} = 
\]
\[
    \begin{bmatrix}
        \cos^2(\theta)+\sin^2(\theta) & 0\\
        0 & \cos^2(\theta)+\sin^2(\theta)
    \end{bmatrix} = 
    \begin{bmatrix}
        1 & 0\\
        0 & 1
    \end{bmatrix}.
 \]
Similarly, one shows that $RR^T=I$.
Therefore, a rotation in $\mathbb R^2$ is an orthonormal transformation.

\subsection*{Exercise 10}
(i) 
Suppose $Q$ represents an orthonormal operator on $\mathbb F^n$.
Then $<x,y>=<Q(x),Q(y)>$ for each $x,y\in\mathbb F^n$.
Since $<Q(x),Q(y)> = (Qx)^H(Qy)=x^HQ^HQy$, it equals $x^Hy$ for all $x,y\in\mathbb F^n$
only if $Q^HQ=I$.
On the other hand if $Q^HQ=QQ^H=I$, then
$<Q(x),Q(y)>=(Qx)^H(Qy)=x^HQ^HQy=x^Hy=<x,y>$.

(ii)
\begin{align*}
    ||Qx||=\sqrt{<Qx,Qx>}=\sqrt{x^HQ^HQx}=\sqrt{<x,x>}=||x||.
\end{align*}

(iii)
If $Q^HQ=QQ^H=I$, then $Q^{-1}=Q^H$.
Since $(Q^H)^H=Q$, $Q^H$ is also orthonormal:
\begin{equation*}
    (Q^H)^HQ^H=QQ^H=I=Q^HQ=Q^H(Q^H)^H.
\end{equation*}

(iv)
Let $q_i$ denote the $i^th$ column of $Q$.
Since $Q$ is orthonormal, 
$(Q^HQ)_{ij}=q_i^Hq_j=<q_i,q_j>$ is $1$ if $i=j$ and $0$ if $i\neq j$.
Thus, the columns of $Q$ are orthonormal.

(v)
The matrix
\[\begin{bmatrix}
    2 & 0\\
    0 & 1/2
\end{bmatrix}\]
shows that the converse is not true.

(vi)
\begin{align*}
    (Q_1Q_2)^HQ_1Q_2=Q_2^HQ_1^HQ_1Q_2=Q_2^HQ_2=I
\end{align*}
and
\begin{align*}
     Q_1Q_2(Q_1Q_2)^H=Q_1Q_2Q_2^HQ_1^H=Q_1Q_1^H=I.
\end{align*}
Therefore, $Q_1Q_2$ is orthonormal.

\subsection*{Exercise 11}
Fix $N\in\mathbb N$, $N>0$, and suppose $\{x_i\}_{i=1}^N$ is a set of
linearly dependent vectors in $V$.
Also, suppose, without loss of generality, that for $2<k<N$,
$\{x_i\}_{i=1}^{k-1}$ is a linearly independent set and $\{x_i\}_{i=1}^k$ is a linearly dependent set.
Then $\{q_i\}_{i=1}^{k-1}$ (as they are defined in the book) is also a linearly independent set.
However, since $x_k\in\text{span}(\{x_i\}_{i=1}^{k-1})$, we have that $q_k=0$.
Therefore the Gram-Schmidt orthonormalization process brakes down.

\subsection*{Exercise 16}
(i)
Let $A\in\mathbb M_{mxn}$ where $\text{rank}(A)=n\leq m$.
Then there exist orthonormal $Q\in\mathbb M_{mxm}$ and
upper triangular $R\in\mathbb M_{mxn}$ such that $A=QR$.
Since $\tilde{Q}=-Q$ is still orthonormal ($-Q(-Q)^H=-Q(-Q^H)=QQ^H=I$ 
and similarly one shows $(-Q)^H(-Q)=I$)
and $\tilde{R}=-R$ is still upper triangular, 
$A=QR=\tilde{Q}\tilde{R}$.
Therefore QR-decomposition is not unique.

(ii)
Suppose now that $A$ is invertible and can be decomposed into 
two different QR decompositions: $QR$ and $\tilde{Q}\tilde{R}$,
where the diagonal entries of $R$ and $\tilde{R}$ are strictly positive.
Then both $R$ and $\tilde{R}$ are invertible and we conclude that
$\tilde{R}^{-1}R=Q^H\tilde{Q}$.
Since $R$ and $\tilde{R}$ are upper triangular, so is the LHS of the previous equation.
On the other hand, since $Q$ and $\tilde{Q}$ are orthonormal, so is the RHS.
Therefore $\tilde{R}^{-1}R=I$ and, by unicity of the inverse, we conclude that $R=\tilde{R}$,
and so $Q=\tilde{Q}$.

\subsection*{Exercise 17}
Take a reduced QR-decomposition $A=\hat{Q}\hat{R}$,
where $\hat{Q}\in\mathbb M_{mxn}$ is orthonormal and $\hat{R}\in\mathbb M_{nxn}$ is upper triangular.
Since $A$ has full column rank, $\hat{R}$ has full rank and is therefore nonsingular.
Then,
\begin{align*}
    &A^HAx=A^Hb\ \implies\\ 
    &(\hat{Q}\hat{R})^H\hat{Q}\hat{R}x = (\hat{Q}\hat{R})^Hb\ \implies\\
    &\hat{R}^H\hat{Q}^H\hat{Q}\hat{R}x = \hat{R}^H\hat{Q}^Hb,
\end{align*}
and premultiplying both LHS and RHS of the last equation by $\hat{R}^{-1}$ gives
$\hat{R}x = \hat{Q}^Hb$.

\subsection*{Exercise 23}
Let $x,y\in V$ and define $v:=-y$.
Since a norm is nonnegative and satisfies the triangular property, 
$||x||-||v||\leq ||x||+||v||\leq||x+v||$.
Then our definition of $v$ implies $||x||-||y||=||x||-||-y||\leq||x-y||$.
Interchanging the role of $x$ and $y$ and using the homogeneity property of norms we have
$||y||-||x||\leq||y-x||=||-(y-x)||=||x-y||$,
and the result follows.

\subsection*{Exercise 24}
(i)
Since $|f(t)|\geq 0$ for every $t$, so is $\int_a^b|f(t)|dt$.
In addition, if $f=0$, then $\int_a^b|f(t)|dt=0$.
On the other hand, if $\int_a^b|f(t)|dt=0$ and $|f(t)|\geq 0$, it must be that
$|f(t)|=0$ for all t, implying that $f=0$.
Now take a constant $c\in\mathbb F$, then 
$\int_a^b|cf(t)|dt=\int_a^b|c||f(t)|dt=|c|\int_a^b|f(t)|dt$,
since $c$ does not depend on $t$.
Finally, take $g\in C([a, b]; \mathbb F)$.
Since $|f(t)+g(t)|\leq|f(t)|+|g(t)|$ for all $t$ and the integral is a linear operator,
we have that $\int_a^b|f(t)+g(t)|dt\leq\int_a^b|f(t)|dt + \int_a^b|g(t)|dt$.

(ii)
Since $|f(t)|^2\geq 0$ for every $t$, so is $\int_a^b|f(t)|^2dt$ and its square root.
In addition, if $f=0$, then $|f(t)|^2=0$ for all $t$ and $\sqrt{\int_a^b|f(t)|^2dt}=0$.
On the other hand, if $\sqrt{\int_a^b|f(t)|^2dt}=0$, 
then $\int_a^b|f(t)|^2dt=0$ and since $|f(t)|^2\geq 0$ for all $t$, 
it must be that $|f(t)|^2=0$ for all t, implying that $f=0$.
Now take a constant $c\in\mathbb F$, then 
$\sqrt{\int_a^b|cf(t)|^2dt}=\sqrt{\int_a^b|c|^2|f(t)|^2dt}=|c|\sqrt{\int_a^b|f(t)|^2dt}$,
since $c$ does not depend on $t$.
Finally, take $g\in C([a, b]; \mathbb F)$.
Since $|f(t)+g(t)|\leq|f(t)|+|g(t)|$ for all $t$, 
$x\mapsto x^2$ and $x\mapsto\sqrt{x}$ are monotonically increasing for nonnegative $x$ 
and the integral is a linear operator,
we have that $\sqrt{\int_a^b|f(t)+g(t)|^2dt}\leq\sqrt{\int_a^b|f(t)|^2dt + \int_a^b|g(t)|^2dt}
\leq||f||_{L2}+||g||_{L2}$.

(iii)
Since $|f(x)|\geq 0$ for all $x$, so is the $\sup_{x\in[a, b]}|f(x)|$.
In addition, if $f=0$, then $\sup_{x\in[a, b]}|f(x)|$ is also zero.
On the other hand, since $|f(x)|\geq 0$ for all $x$, $0\leq\sup_{x\in[a, b]}|f(x)|=0$
implies that we must have $f=0$.
Now take a constant $c\in\mathbb F$, then
$\sup_{x\in[a, b]}|cf(x)|=\sup_{x\in[a, b]}|c||f(x)|=|c|\sup_{x\in[a, b]}|f(x)|$.
Finally, take $g\in C([a, b];\mathbb F)$.
Since $|f(x)+g(x)|\leq|f(x)|+|g(x)|$ for all $x$, we have that
$\sup_{x\in[a, b]}|f(x)+g(x)|\leq\sup_{x\in[a, b]}\{|f(x)|+|g(x)|\}
\leq\sup_{x\in[a, b]}|f(x)|+\sup_{x\in[a, b]}|g(x)|$.

\subsection*{Exercise 26}
We show that topological equivalence is an equivalence relation.
Let $||\cdot||_r$ be a norm on $X$ for $r\in\{a, b, c\}$.
Clearly $||\cdot||_r$ is in topologically equivalent with itself,
just pick any $0<m\leq 1$ and any $M\geq 1$ to show this.
Also, suppose that $||\cdot||_a$ is topologically equivalent to $||\cdot||_b$
with constants $0<m\leq M$.
Then, $||\cdot||_b$ is topologically equivalent to $||\cdot||_a$ with constants
$0<1/M'\leq 1/m'$.
Finally, if $||\cdot||_a$ is topologically equivalent to $||\cdot||_b$ with constants
$0<m\leq M$ and so is $||\cdot||_b$ with $||\cdot||_c$ with constants $0<m'\leq M'$,
then $||\cdot||_a$ is topologically equivalent to $||\cdot||_b$ with constants
$0<mm'\leq MM'$.

Take $x\in\mathbb R^n$
Notice that
\begin{equation*}
    \sum_{i=1}^n|x_i|^2\leq
    \left(\sum_{i=1}^n|x_i|^2+2\sum_{i\neq j}|x_i||x_j|\right)=
    \left(\sum_{i=1}^n|x_i|\right)^2
\end{equation*}
and that
\begin{equation*}
    \sum_{i=1}^n|x_i|\cdot1\leq
    \left(\sum_{i=1}^n|x_i|^2\right)^{1/2}\left(\sum_{i=1}^n1^2\right)^{1/2}=
    \sqrt{n}\left(\sum_{i=1}^n|x_i|^2\right)^{1/2}
\end{equation*}
prove that $||x||_2\leq||x||_1\leq\sqrt{n}||x||_2$.

Also notice that
\begin{equation*}
    \max_{i}|x_i|=\left(\max_i|x_i|^2\right)^{1/2}\leq
    \left(\sum_{i=1}^n|x_i|^2\right)^{1/2}=
\end{equation*}
and
\begin{equation*}
    \sum_{i=1}^n|x_i|^2\leq n\cdot\max_i|x_i|^2
\end{equation*}
prove that $||x||_\infty\leq||x||_2\leq \sqrt{n}||x||_\infty$.

\subsection*{Exercise 28}
(i)
Notice that (applying the results of the previous exercise)
\begin{align*}
    \sup_{x\neq 0}\frac{||Ax||_1}{||x||_1}\leq
    \sup_{x\neq 0}\frac{||Ax||_1}{||x||_1}\leq
    \sqrt{n}\sup_{x\neq 0}\frac{||Ax||_2}{||x||_2},
\end{align*}
and
\begin{align*}
    \sup_{x\neq 0}\frac{||Ax||_1}{||x||_1}\geq
    \sup_{x\neq 0}\frac{||Ax||_2}{||x||_1}\geq
    \frac{1}{\sqrt{n}}\sup_{x\neq 0}\frac{||Ax||_2}{||x||_2}
\end{align*}
imply that $\frac{1}{\sqrt{n}}||A||_2\leq||A||_1\leq||A||_2$.

(ii)
Notice that
\begin{equation*}
    \sup_{x\neq 0}\frac{||Ax||_2}{||x||_2}\leq
    \sup_{x\neq 0}\frac{\sqrt{n}||Ax||_\infty}{||x||_\infty},
\end{equation*}
and
\begin{equation*}
    \sup_{x\neq 0}\frac{||Ax||_2}{||x||_2}\geq
    \sup_{x\neq 0}\frac{||Ax||_\infty}{\sqrt{n}||x||_\infty}.
\end{equation*}

\subsection*{Exercise 29}
Take an arbitrary $x\neq 0$ and suppose $||\cdot||$ is an inner product induced norm.
Since
\begin{equation*}
    ||Qx||=\left(<Qx,Qx>\right)^{1/2}=
    \left(<Q^HQx,x>\right)^{1/2}=
    \left(<x,x>\right)^{1/2}=
    ||x||,
\end{equation*}
then 
\begin{equation*}
    ||Q||=\sup_{x\neq 0}\frac{||Qx||}{||x||}=1.
\end{equation*}

Now let $R_x:\mathbb M_n(\mathbb F)\to\mathbb F^n, A\mapsto Ax$ for every $x\in\mathbb F^n$.
Notice that
\begin{equation*}
    ||R_x||=\sup_{A\neq 0}\frac{||Ax||}{||A||}=
    \sup_{A\neq 0}\frac{||Ax||||x||}{||A||||x||}\leq
    \sup_{A\neq 0}\left(\frac{||Ax||||x||}{||Ax||}\right)=
    ||x||.
\end{equation*}


\subsection*{Exercise 30}
Take arbitrary matrices $A,B\in\mathbb M_n(\mathbb F)$.
First, $||A||_S=||SAS^{-1}||\geq 0$ for any $A$ because 
$||\cdot||$ is a norm on $\mathbb M_n(\mathbb F)$ and 
$SAS^{-1}\in\mathbb M_n(\mathbb F)$.
In addition, $||0||_S=||S0S^-1||=||0||=0$ and if
$0=||A||_S=||SAS^{-1}||$, then $SAS^{-1}=0$ which implies $A=0$.
Second, take $a\in\mathbb F$, then 
\begin{equation*}
    ||aA||_S=||SaAS^{-1}||=
    ||aSAS^{-1}||=|a|||SAS^{-1}||=
    |a|||A||_S.
\end{equation*}
Finally, let $B\in\mathbb M_n(\mathbb F)$ and notice that
\begin{equation*}
    ||A+B||_S=||S(A+B)S^{-1}||=
    ||SAS^{-1}+SBS^{-1}||\leq||SAS^{-1}||+||SBS^{-1}||=
    ||A||_S+||B||_S.
\end{equation*}
Therefore $||\cdot||_S$ is a norm on $\mathbb M_n(\mathbb F)$.
To show that it is a matrix norm, notice that
\begin{align*}
    ||AB||_S=||SABS^{-1}||=
    ||SAS^{-1}ABS^{-1}||\leq
    ||SAS^{-1}||||SBS^{-1}||,
\end{align*}
and so $||AB||_S\leq||A||_S||B||_S$.

\subsection*{Exercise 37}
Since $V:=\mathbb R[x; 2]$ is isomorphic to $\mathbb R^3$,
we can represent an arbitrary element $p\in V$, $p=ax^2+bx+c$,
as a vector on $\mathbb R^3$, $p=(a, b, c)$.
Then we need to find a vector $q=(a', b', c')$ such that
$p'q=2a+b=p'(1)=L[p]$.
Thus, $q=(2, 1, 0)$.

\subsection*{Exercise 38}
Let $p=ax^2+vx+c$ be an arbitrary element of $V=\mathbb F[x;2]$.
Since we can represent $p=(a, b, c)^T$, and $p'=D(p)=(0,2a,b)^T$,
we that the matrix representation of $D$ is
\begin{equation*}
    D = \begin{bmatrix}
        0 & 0 & 0\\
        2 & 0 & 0\\
        0 & 1 & 0
    \end{bmatrix},
\end{equation*}
and the hermitian is just the transpose
\begin{equation*}
    D = \begin{bmatrix}
        0 & 2 & 0\\
        0 & 0 & 1\\
        0 & 0 & 0
    \end{bmatrix}.
\end{equation*}

\subsection*{Exercise 39}
(i)
By definition of adjoint and linearity of inner products,
\begin{align*}
    &<(S+T)^*w,v>_V=
    <w,(S+T)v>_W=\\
    &<w,Sv+Tv>_W=
    <w,Sv>_W+<w,Tv>_W=\\
    &<S^*w,v>_V + <T^*w,v>_V=
    <S^*w+T^*w,v>_V.
\end{align*}
Then $(S+T)^*=S^*+T^*$.
Also,
\begin{align*}
    &<(\alpha T)^*w,v>_V=
    <w,(\alpha T)v>_W=\\
    &<w,\alpha Tv>_W=
    \alpha<w, Tv>=\\
    &\alpha<T^*w,v>=
    <\bar{\alpha}T^*w,v>,
\end{align*}
thus $(\alpha T)^*=\bar{\alpha}T$.

(ii)
By the definition of adjoint of $S$ and the properties of inner products we have that
\begin{align*}
    <w,Sv>_W=<S^*w,v>_V=
    \overline{<v,S^*w>_V}=\overline{<S^{**}v,w>_W}=
    <w,S^{**}v>_W
\end{align*}
for all $v\in V$ and $w\in W$.
Therefore $S=S**$.

(iii)
By the definition of adjoint we have
\begin{align*}
    &<(ST)^*v',v>_V=<v',(ST)v>_V=<v',S(Tv)>_V=\\
    &<S^*v',Tv>_V=<T*S*v',v>_V,
\end{align*}
thereby proving that $(ST)^*=T^*S^*$.

(iv)
Using (iii) we have $T^*(T^*)^{-1}=(TT^{-1})^*=I^*=I$.

\subsection*{Exercise 40}
(i)
Let $B,C\in\mathbb M_n(\mathbb F)$.
By definition of Frobenious inner product
\begin{align*}
    <B,AC>_F=\text{tr}(B^HAC)=\text{tr}((A^HB)^HC)=<A^HB,C>_F.
\end{align*}

(ii)
By definition of Frobenious norm and the properties of the trace we have
\begin{align*}
    <A_2,A_3A_1>_F=\text{tr}(A_2^HA_3A_1)=
    \text{tr}(A_1A_2^HA_3)=\text{tr}((A_2A_1^H)^HA_3)=
    <A_2A_1^H,A_3>_F=<A_2A_1^*,A_3>.
\end{align*}

(iii)
Given $B,C\in\mathbb M_n(\mathbb F)$, we have $<B,AC-CA>=<B,AC>-<B,CA>$. 
Applying (ii) to the second term we get $<B,CA>=<BA^*,C>$.
On the other hand, 
\begin{equation*}
    <B,AC>=\text{tr}(B^HAC)=\text{tr}((A^HB)^HC)=<A^HB,C>=<A^*B,C>.
\end{equation*}
Putting all together we obtain that $T_A^*=T_{A^*}$.

\subsection*{Exercise 44}
Suppose there exists an $x\in\mathbb F^n$ such that $Ax=b$.
Then, for every $y\in\mathcal N(A^H)$, 
$$<y,b>=<y,Ax>=<A^Hy,x>=<0,x>=0.$$
Now suppose that there exists a $y\in\mathcal N(A^H)$ such that $<y,b>\neq0$.
Then $b\notin\mathcal N(A^H)^\perp=\mathcal R(A)$.
Therefore for no $x\in\mathbb F^n$, $Ax=b$.

\subsection*{Exercise 45}
Let $A\in\text{Sym}_n(\mathbb R)$ and $B\in\text{Skew}_n(\mathbb R)$.
Then 
\begin{align*}
    <B,A>=\text{Tr}(B^TA)=\text{Tr}(AB^T)=
    \text{Tr}(A^T(-B))=-<A,B>.
\end{align*}
We conclude that $<A,B>=0$ and $\text{Skew}_n(\mathbb R)\subset\text{Sym}_n(\mathbb R)^\perp$.
Now suppose $B\in\text{Sym}_n(\mathbb R)^\perp$.
As for any other matrix, $B+B^T\in\text{Sym}_n(\mathbb R)$.
Thus,
\begin{align*}
    0 = <B+B^T,B>=\text{Tr}((B+B^T)B) =\text{Tr}(BB + B^TB)=
    \text{Tr}(BB)+\text{Tr}(B^TB),
\end{align*}
which implies $<B^T,B>=<-B,B>$ and so $B^T=-B$.
Therefore $\text{Sym}_n(\mathbb R)^\perp=\text{Skew}_n(\mathbb R)$.

\subsection*{Exercise 46}
(i)
if $x\in\mathcal N(A^HA)$, $0=(A^HA)x=A^H(Ax)$ and $Ax\in\mathcal N(A^H)$.
Also, $Ax$ is in the range of $A$ by definition.

(ii)
Suppose $x\in\mathcal N(A)$.
Then $Ax=0$.
Premultiplying by $A^H$ both sides of the equation we obtain $A^HAx=A^H0=0$
and so $x\in\mathcal N(A^HA)$.
On the other hand, suppose $x\in\mathcal N(A^HA)$.
Then $||Ax||=x^HA^HAx=x^H0=0$, so that $Ax=0$ and $x\in\mathcal N(A)$

(iii)
By the rank-nullity theorem we have $n=\text{Rank}(A)+\text{Dim}\mathcal N(A)$
and $n=\text{Rank}(A^HA)+\text{Dim}\mathcal N(A^HA)$.
Then by (ii) it follows that $\text{Rank}(A)=\text{Rank}(A^HA)$.

(iv)
By (iii) and the assumption on $A$ we have that $n=\text{Rank}(A)=\text{Rank}(A^HA)$.
Since $A^HA\in\mathbb M_n$, it is nonsingular.

\subsection*{Exercise 47}
(i)
Notice that
\begin{align*}
    P^2=(A(A^HA)^{-1}A^H)(A(A^HA)^{-1}A^H)=
    A(A^HA)^{-1}A^HA(A^HA)^{-1}A^H=
    A(A^HA)^{-1}A^H=P.
\end{align*}

(ii)
Notice that
\begin{align*}
    P^H=(A(A^HA)^{-1}A^H)^H=
    (A^H)^H(A^HA)^{-H}A^H=A(A^HA)^{-1}A^H=P.
\end{align*}

(iii)
$A$ has rank $n$, therefore $P$ has at most rank $n$.
Take $y$ in the range of $A$.
Then there exists an $x\in\mathbb F^n$ such that $y=Ax$.
Then
\begin{align*}
    Py=A(A^HA)A^Hy=A(A^HA)^{-1}A^HAx=Ax=y
\end{align*}
shows that $y$ is also in the range of $P$.
Therefore $\text{Rank}(P)\geq\text{Rank}(A)$ and so $P$ has rank $p$

\subsection*{Exercise 48}
(i)
Let $A,B\in\mathbb M_n(\mathbb R)$ and $x\in\mathbb R$.
Then
\begin{align*}
    P(A+xB)=\frac{(A+xB)+(A+xB)^T}{2}=
    \frac{A+A^T+x(B+B^T)}{2}=P(A)+xP(B).
\end{align*}
Thus $P$ is a linear transformation.

(ii)
Now notice that
\begin{align*}
    P^2(A)=\frac{\frac{A+A^T}{2}+\frac{A^T+A}{2}}{2}=
    \frac{\frac{2A+2A^T}{2}}{2}=\frac{2A+2A^T}{2}=P(A).
\end{align*}

(iii)
By definition of adjoint we have $<P^*(A),B>=<A,P(B)>$.
Then, notice that
\begin{align*}
    &<A,P(B)>=<A,(B+B^T)/2>=
    <A,B/2>+<A,B^T/2>=\\
    &\text{Tr}(A^TB/2)+\text{Tr}(A^TB^T/2)=
    \text{Tr}(A^T/2B)+\text{Tr}(BA/2)=\\
    &\text{Tr}(A^T/2B)+\text{Tr}(A/2B)=
    <(A+A^T)/2,B>=<P(A),B>.
\end{align*}
Thus $P=P^*$.

(iv)
Suppose $A\in\mathcal N(P)$. 
Then $0=P(A)=(A+A^T)/2$ implies $A^T=-A$, thus $\mathcal N(P)\subset\text{Skew}(\mathbb R)$.
Now suppose $A\in\text{Skew}(\mathbb R)$.
Then $A^T=-A$ and so $P(A)=(A+A^T)/2=0$. Thus $\text{Skew}(\mathbb R)\subset\mathcal N(P)$.

(v)
Let $A\in\mathbb M_n(\mathbb R)$.
Then $P(A)=(A+A^T)/2=(A^T+A)/2=P(A)^T$ and so $\mathcal R(P)=\text{Sym}(\mathbb R)$.
Now let $A=\text{Sym}(\mathbb R)$.
Thus $A=A^T$ and $P(A)=(A+A^T)/2=(A+A)/2=A$ and so $A\in\mathcal R(P)$.
This shows that $\mathcal R(P)=\text{Sym}(\mathbb R)$.

(vi)
Notice that
\begin{align*}
    &||A - P(A)||_F^2 = <A - P(A), A - P(A)> =
    <A - \frac{A + A^T}{2}, A - \frac{A + A^T}{2}> =\\
    &<\frac{A - A^T}{2}, \frac{A - A^T}{2}> = 
    \text{Tr}\left(\left(\frac{A - A^T}{2}\right)^T\frac{A - A^T}{2}\right)=\\
    &\text{Tr}\left(\frac{A^T - A}{2}\frac{A - A^T}{2}\right) = 
    \text{Tr}\left(\frac{A^TA - A^2 - (A^T)^2 + AA^T}{4}\right) =\\ 
    &\text{Tr}\left(\frac{A^TA - A^2 - A^2 + A^TA}{4}\right) =
    \text{Tr}\left(\frac{A^TA - A^2}{2}\right) = 
    \frac{\text{Tr}(A^TA) - \text{Tr}(A^2)}{2}.
\end{align*}
Therefore $||A - P(A)||_F = \sqrt{\frac{\text{Tr}(A^TA) - \text{Tr}(A^2)}{2}}$.

\subsection*{Exercise 50}
We want to estimate $y^2=1/s+rx^2/s$ via OLS.
We rewrite the model in the form $Ax=b$ where
$b_i=y_i^2$, $A_i=(1\ x_i)$ and $x=(\beta_1\ \beta_2)^T$ where $\beta_1=1/s$ and $\beta_2=r/s$.
Then the normal equations are $A^HA\hat{x}=A^Hb$, where
\begin{align*}
    A^HA\hat{x} =
    \begin{bmatrix}
        \sum_i 1 & \sum_ix_i^2\\
        \sum_ix_i^2& \sum_ix_i^4
    \end{bmatrix} 
    \begin{bmatrix}
        \hat{\beta}_1\\ \hat{\beta}_2
    \end{bmatrix} =
    \begin{bmatrix}
        n\hat{\beta}_1 - \hat{\beta}_2\sum_i x_i^2\\
        \hat{\beta}_1\sum_ix_i^2 - \hat{\beta}_2\sum_ix_i^4
    \end{bmatrix}
\end{align*}
and
\begin{align*}
    A^Hb=
    \begin{bmatrix}
        \sum_i y_i^2\\
        \sum_i x_i^2y_i^2
    \end{bmatrix}.
\end{align*}

\end{document}
